home_teams2 <- teams2[grep("@",teams2)]
#away_teams2 <- teams2[!grepl("@", teams2)]
home_teams2 <- gsub("@","",home_teams2)
covers_html <- html_nodes(html_page2, ".covers_top")
covers <- html_text(covers_html)
covers <- gsub("[\r\n]", "", covers)
covers_df <- data.frame(covers)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
# To use one of these functions on myDummy$country:
covers_df$covers <- trim(covers_df$covers)
covers_df <- Test <- separate(covers_df, col = "covers", into = c("away", "home"), sep = "\\s+")
x <- cbind.data.frame(home_teams, away_teams, pick_pct_home)
x$home_teams <- trim(x$home_teams)
x$pick_pct_home <- as.numeric(gsub("\\%", "", x$pick_pct_home))/100
x$pick_pct_away <- 1 - x$pick_pct_home
y <- cbind.data.frame(home_teams2, covers_df)
names(y)[names(y)=="home_teams2"] <- "home_teams"
y$home_teams <- gsub("N.Y. Jets", "New York (NYJ)", y$home_teams)
y$home_teams <- gsub("N.Y. Giants", "New York (NYG)", y$home_teams)
y$home <- gsub("OFF", -100, y$home)
y$away <- gsub("OFF", -100, y$away)
z <- merge(x,y,by = "home_teams")
winprob <- function(x){
ifelse(x <= 0, -x / (100-x), 100/(100+x))
}
z$home_prob <- winprob(as.numeric(as.character(z$home)))
z$away_prob <- winprob(as.numeric(as.character(z$away)))
z$prob_total <- z$home_prob + z$away_prob
z$home_prob <- z$home_prob / z$prob_total
z$away_prob <- z$away_prob / z$prob_total
z$fav_prob <- pmax(z$home_prob, z$away_prob)
z <- z[order(-z$fav_prob),]
z$pick_pct_fav <- ifelse(z$home_prob >=.5, z$pick_pct_home, z$pick_pct_away)
z$fav <- ifelse(z$home_prob >=.5, as.character(z$home_teams), as.character(z$away_teams))
z$confidence <- seq(16,17-length(z$fav))
z <- select(z,confidence, fav, fav_prob, pick_pct_fav)
z
write.csv(z, file = "Picks2.csv", row.names = FALSE)
source('~/pickem2.R')
library(rvest)
library(tidyr)
library(dplyr)
web_page1 <- "https://football.fantasysports.yahoo.com/pickem/pickdistribution?gid=&week=12&type=c"
web_page2 <- "http://www.covers.com/odds/football/nfl-moneyline-odds.aspx"
html_page1 <- read_html(web_page1)
html_page2 <- read_html(web_page2)
# Read in data from yahoo
pick_pct_html <- html_nodes(html_page1, "dd")
pick_pct <- html_text(pick_pct_html)
even_index <- seq(2,length(pick_pct),2)
odd_index <- seq(1, length(pick_pct)-1, 2)
teams <- pick_pct[odd_index]
home_teams <- teams[grep("@",teams)]
away_teams <- teams[!grepl("@ ", teams)]
pick_pct <- pick_pct[even_index]
pick_pct_home <- pick_pct[match(home_teams,teams)]
home_teams <- gsub("@ ","",home_teams)
teams2_html <- html_nodes(html_page2, "strong")
teams2 <- html_text(teams2_html)
length(teams2) <- length(teams2) - 1
home_teams2 <- teams2[grep("@",teams2)]
#away_teams2 <- teams2[!grepl("@", teams2)]
home_teams2 <- gsub("@","",home_teams2)
covers_html <- html_nodes(html_page2, ".covers_top")
covers <- html_text(covers_html)
covers <- gsub("[\r\n]", "", covers)
covers_df <- data.frame(covers)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
# To use one of these functions on myDummy$country:
covers_df$covers <- trim(covers_df$covers)
covers_df <- Test <- separate(covers_df, col = "covers", into = c("away", "home"), sep = "\\s+")
x <- cbind.data.frame(home_teams, away_teams, pick_pct_home)
x$home_teams <- trim(x$home_teams)
x$pick_pct_home <- as.numeric(gsub("\\%", "", x$pick_pct_home))/100
x$pick_pct_away <- 1 - x$pick_pct_home
y <- cbind.data.frame(home_teams2, covers_df)
names(y)[names(y)=="home_teams2"] <- "home_teams"
y$home_teams <- gsub("N.Y. Jets", "New York (NYJ)", y$home_teams)
y$home_teams <- gsub("N.Y. Giants", "New York (NYG)", y$home_teams)
y$home <- gsub("OFF", -100, y$home)
y$away <- gsub("OFF", -100, y$away)
z <- merge(x,y,by = "home_teams")
winprob <- function(x){
ifelse(x <= 0, -x / (100-x), 100/(100+x))
}
z$home_prob <- winprob(as.numeric(as.character(z$home)))
z$away_prob <- winprob(as.numeric(as.character(z$away)))
z$prob_total <- z$home_prob + z$away_prob
z$home_prob <- z$home_prob / z$prob_total
z$away_prob <- z$away_prob / z$prob_total
z$fav_prob <- pmax(z$home_prob, z$away_prob)
z <- z[order(-z$fav_prob),]
z$pick_pct_fav <- ifelse(z$home_prob >=.5, z$pick_pct_home, z$pick_pct_away)
z$fav <- ifelse(z$home_prob >=.5, as.character(z$home_teams), as.character(z$away_teams))
z$confidence <- seq(16,17-length(z$fav))
z <- select(z,confidence, fav, fav_prob, pick_pct_fav)
z
write.csv(z, file = "Picks2.csv", row.names = FALSE)
library(rvest)
library(tidyr)
library(dplyr)
web_page1 <- "https://football.fantasysports.yahoo.com/pickem/pickdistribution?gid=&week=12&type=c"
web_page2 <- "http://www.covers.com/odds/football/nfl-moneyline-odds.aspx"
html_page1 <- read_html(web_page1)
html_page2 <- read_html(web_page2)
# Read in data from yahoo
pick_pct_html <- html_nodes(html_page1, "dd")
pick_pct <- html_text(pick_pct_html)
even_index <- seq(2,length(pick_pct),2)
odd_index <- seq(1, length(pick_pct)-1, 2)
teams <- pick_pct[odd_index]
home_teams <- teams[grep("@",teams)]
away_teams <- teams[!grepl("@ ", teams)]
pick_pct <- pick_pct[even_index]
pick_pct_home <- pick_pct[match(home_teams,teams)]
home_teams <- gsub("@ ","",home_teams)
teams2_html <- html_nodes(html_page2, "strong")
teams2 <- html_text(teams2_html)
length(teams2) <- length(teams2) - 1
home_teams2 <- teams2[grep("@",teams2)]
#away_teams2 <- teams2[!grepl("@", teams2)]
home_teams2 <- gsub("@","",home_teams2)
covers_html <- html_nodes(html_page2, ".covers_top")
covers <- html_text(covers_html)
covers <- gsub("[\r\n]", "", covers)
covers_df <- data.frame(covers)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
# To use one of these functions on myDummy$country:
covers_df$covers <- trim(covers_df$covers)
covers_df <- Test <- separate(covers_df, col = "covers", into = c("away", "home"), sep = "\\s+")
x <- cbind.data.frame(home_teams, away_teams, pick_pct_home)
x$home_teams <- trim(x$home_teams)
x$pick_pct_home <- as.numeric(gsub("\\%", "", x$pick_pct_home))/100
x$pick_pct_away <- 1 - x$pick_pct_home
y <- cbind.data.frame(home_teams2, covers_df)
names(y)[names(y)=="home_teams2"] <- "home_teams"
y$home_teams <- gsub("N.Y. Jets", "New York (NYJ)", y$home_teams)
y$home_teams <- gsub("N.Y. Giants", "New York (NYG)", y$home_teams)
y$home <- gsub("OFF", -100, y$home)
y$away <- gsub("OFF", -100, y$away)
z <- merge(x,y,by = "home_teams")
winprob <- function(x){
ifelse(x <= 0, -x / (100-x), 100/(100+x))
}
z$home_prob <- winprob(as.numeric(as.character(z$home)))
z$away_prob <- winprob(as.numeric(as.character(z$away)))
z$prob_total <- z$home_prob + z$away_prob
z$home_prob <- z$home_prob / z$prob_total
z$away_prob <- z$away_prob / z$prob_total
z$fav_prob <- pmax(z$home_prob, z$away_prob)
z <- z[order(-z$fav_prob),]
z$pick_pct_fav <- ifelse(z$home_prob >=.5, z$pick_pct_home, z$pick_pct_away)
z$fav <- ifelse(z$home_prob >=.5, as.character(z$home_teams), as.character(z$away_teams))
z$confidence <- seq(16,17-length(z$fav))
z <- select(z,confidence, fav, fav_prob, pick_pct_fav)
z
write.csv(z, file = "Picks2.csv", row.names = FALSE)
library(rvest)
library(tidyr)
library(dplyr)
web_page1 <- "https://football.fantasysports.yahoo.com/pickem/pickdistribution?gid=&week=12&type=c"
web_page2 <- "http://www.covers.com/odds/football/nfl-moneyline-odds.aspx"
html_page1 <- read_html(web_page1)
html_page2 <- read_html(web_page2)
# Read in data from yahoo
pick_pct_html <- html_nodes(html_page1, "dd")
pick_pct <- html_text(pick_pct_html)
even_index <- seq(2,length(pick_pct),2)
odd_index <- seq(1, length(pick_pct)-1, 2)
teams <- pick_pct[odd_index]
home_teams <- teams[grep("@",teams)]
away_teams <- teams[!grepl("@ ", teams)]
pick_pct <- pick_pct[even_index]
pick_pct_home <- pick_pct[match(home_teams,teams)]
home_teams <- gsub("@ ","",home_teams)
teams2_html <- html_nodes(html_page2, "strong")
teams2 <- html_text(teams2_html)
length(teams2) <- length(teams2) - 1
home_teams2 <- teams2[grep("@",teams2)]
#away_teams2 <- teams2[!grepl("@", teams2)]
home_teams2 <- gsub("@","",home_teams2)
covers_html <- html_nodes(html_page2, ".covers_top")
covers <- html_text(covers_html)
covers <- gsub("[\r\n]", "", covers)
covers_df <- data.frame(covers)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
# To use one of these functions on myDummy$country:
covers_df$covers <- trim(covers_df$covers)
covers_df <- Test <- separate(covers_df, col = "covers", into = c("away", "home"), sep = "\\s+")
x <- cbind.data.frame(home_teams, away_teams, pick_pct_home)
x$home_teams <- trim(x$home_teams)
x$pick_pct_home <- as.numeric(gsub("\\%", "", x$pick_pct_home))/100
x$pick_pct_away <- 1 - x$pick_pct_home
y <- cbind.data.frame(home_teams2, covers_df)
names(y)[names(y)=="home_teams2"] <- "home_teams"
y$home_teams <- gsub("N.Y. Jets", "New York (NYJ)", y$home_teams)
y$home_teams <- gsub("N.Y. Giants", "New York (NYG)", y$home_teams)
y$home <- gsub("OFF", -100, y$home)
y$away <- gsub("OFF", -100, y$away)
z <- merge(x,y,by = "home_teams")
winprob <- function(x){
ifelse(x <= 0, -x / (100-x), 100/(100+x))
}
z$home_prob <- winprob(as.numeric(as.character(z$home)))
z$away_prob <- winprob(as.numeric(as.character(z$away)))
z$prob_total <- z$home_prob + z$away_prob
z$home_prob <- z$home_prob / z$prob_total
z$away_prob <- z$away_prob / z$prob_total
z$fav_prob <- pmax(z$home_prob, z$away_prob)
z <- z[order(-z$fav_prob),]
z$pick_pct_fav <- ifelse(z$home_prob >=.5, z$pick_pct_home, z$pick_pct_away)
z$fav <- ifelse(z$home_prob >=.5, as.character(z$home_teams), as.character(z$away_teams))
z$confidence <- seq(16,17-length(z$fav))
z <- select(z,confidence, fav, fav_prob, pick_pct_fav)
z
write.csv(z, file = "Picks2.csv", row.names = FALSE)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
head(vowel.train)
library(caret)
mod_rf <- train(y ~ ., data = vowel.train, method = "rf")
mod_gbm <- train(y ~ ., data = vowel.train, method = "gbm")
install.packages("gbm")
source('~/pickem2.R')
# Question 1
library(caret)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
mod_rf <- train(y ~ ., data = vowel.train, method = "rf")
mod_gbm <- train(y ~ ., data = vowel.train, method = "gbm")
confusionMatrix(predict(vowel.fit.rf), vowel.train$y)
confusionMatrix(predict(mod_rf), vowel.train$y)
confusionMatrix(predict(mod_rf, vowel.test), vowel.test$y)
confusionMatrix(predict(vowel.fit.gbm), vowel.train$y)
vowel.fit.gbm <- train(y ~ ., data = vowel.train, method = "gbm", verbose = F)
confusionMatrix(predict(vowel.fit.gbm), vowel.train$y)
confusionMatrix(predict(vowel.fit.gbm, vowel.test), vowel.test$y)
confusionMatrix(
predict(vowel.fit.rf, vowel.test),
predict(vowel.fit.gbm, vowel.test)
)
vowel.fit.rf <- train(y ~ ., data = vowel.train, method = "rf")
confusionMatrix(
predict(vowel.fit.rf, vowel.test),
predict(vowel.fit.gbm, vowel.test)
)
confusionMatrix(predict(vowel.fit.rf), vowel.train$y)$overall[1]
confusionMatrix(predict(vowel.fit.rf, vowel.test), vowel.test$y)$overall[1]
library(rvest)
library(tidyr)
library(dplyr)
web_page1 <- "https://football.fantasysports.yahoo.com/pickem/pickdistribution?gid=&week=12&type=c"
web_page2 <- "http://www.covers.com/odds/football/nfl-moneyline-odds.aspx"
html_page1 <- read_html(web_page1)
html_page2 <- read_html(web_page2)
# Read in data from yahoo
pick_pct_html <- html_nodes(html_page1, "dd")
pick_pct <- html_text(pick_pct_html)
even_index <- seq(2,length(pick_pct),2)
odd_index <- seq(1, length(pick_pct)-1, 2)
teams <- pick_pct[odd_index]
home_teams <- teams[grep("@",teams)]
away_teams <- teams[!grepl("@ ", teams)]
pick_pct <- pick_pct[even_index]
pick_pct_home <- pick_pct[match(home_teams,teams)]
home_teams <- gsub("@ ","",home_teams)
teams2_html <- html_nodes(html_page2, "strong")
teams2 <- html_text(teams2_html)
length(teams2) <- length(teams2) - 1
home_teams2 <- teams2[grep("@",teams2)]
#away_teams2 <- teams2[!grepl("@", teams2)]
home_teams2 <- gsub("@","",home_teams2)
covers_html <- html_nodes(html_page2, ".covers_top")
covers <- html_text(covers_html)
covers <- gsub("[\r\n]", "", covers)
covers_df <- data.frame(covers)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
# To use one of these functions on myDummy$country:
covers_df$covers <- trim(covers_df$covers)
covers_df <- Test <- separate(covers_df, col = "covers", into = c("away", "home"), sep = "\\s+")
x <- cbind.data.frame(home_teams, away_teams, pick_pct_home)
x$home_teams <- trim(x$home_teams)
x$pick_pct_home <- as.numeric(gsub("\\%", "", x$pick_pct_home))/100
x$pick_pct_away <- 1 - x$pick_pct_home
y <- cbind.data.frame(home_teams2, covers_df)
names(y)[names(y)=="home_teams2"] <- "home_teams"
y$home_teams <- gsub("N.Y. Jets", "New York (NYJ)", y$home_teams)
y$home_teams <- gsub("N.Y. Giants", "New York (NYG)", y$home_teams)
y$home <- gsub("OFF", -100, y$home)
y$away <- gsub("OFF", -100, y$away)
z <- merge(x,y,by = "home_teams")
winprob <- function(x){
ifelse(x <= 0, -x / (100-x), 100/(100+x))
}
z$home_prob <- winprob(as.numeric(as.character(z$home)))
z$away_prob <- winprob(as.numeric(as.character(z$away)))
z$prob_total <- z$home_prob + z$away_prob
z$home_prob <- z$home_prob / z$prob_total
z$away_prob <- z$away_prob / z$prob_total
z$fav_prob <- pmax(z$home_prob, z$away_prob)
z <- z[order(-z$fav_prob),]
z$pick_pct_fav <- ifelse(z$home_prob >=.5, z$pick_pct_home, z$pick_pct_away)
z$fav <- ifelse(z$home_prob >=.5, as.character(z$home_teams), as.character(z$away_teams))
z$confidence <- seq(16,17-length(z$fav))
z <- select(z,confidence, fav, fav_prob, pick_pct_fav)
z
write.csv(z, file = "Picks2.csv", row.names = FALSE)
html_page1 <- read_html(web_page1)
html_page2 <- read_html(web_page2)
# Read in data from yahoo
pick_pct_html <- html_nodes(html_page1, "dd")
pick_pct <- html_text(pick_pct_html)
even_index <- seq(2,length(pick_pct),2)
odd_index <- seq(1, length(pick_pct)-1, 2)
teams <- pick_pct[odd_index]
home_teams <- teams[grep("@",teams)]
away_teams <- teams[!grepl("@ ", teams)]
pick_pct <- pick_pct[even_index]
pick_pct_home <- pick_pct[match(home_teams,teams)]
home_teams <- gsub("@ ","",home_teams)
teams2_html <- html_nodes(html_page2, "strong")
teams2 <- html_text(teams2_html)
length(teams2) <- length(teams2) - 1
home_teams2 <- teams2[grep("@",teams2)]
#away_teams2 <- teams2[!grepl("@", teams2)]
home_teams2 <- gsub("@","",home_teams2)
covers_html <- html_nodes(html_page2, ".covers_top")
covers <- html_text(covers_html)
covers <- gsub("[\r\n]", "", covers)
covers_df <- data.frame(covers)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
# To use one of these functions on myDummy$country:
covers_df$covers <- trim(covers_df$covers)
covers_df <- Test <- separate(covers_df, col = "covers", into = c("away", "home"), sep = "\\s+")
x <- cbind.data.frame(home_teams, away_teams, pick_pct_home)
x$home_teams <- trim(x$home_teams)
x$pick_pct_home <- as.numeric(gsub("\\%", "", x$pick_pct_home))/100
x$pick_pct_away <- 1 - x$pick_pct_home
y <- cbind.data.frame(home_teams2, covers_df)
names(y)[names(y)=="home_teams2"] <- "home_teams"
y$home_teams <- gsub("N.Y. Jets", "New York (NYJ)", y$home_teams)
y$home_teams <- gsub("N.Y. Giants", "New York (NYG)", y$home_teams)
y$home <- gsub("OFF", -100, y$home)
y$away <- gsub("OFF", -100, y$away)
z <- merge(x,y,by = "home_teams")
winprob <- function(x){
ifelse(x <= 0, -x / (100-x), 100/(100+x))
}
z$home_prob <- winprob(as.numeric(as.character(z$home)))
z$away_prob <- winprob(as.numeric(as.character(z$away)))
z$prob_total <- z$home_prob + z$away_prob
z$home_prob <- z$home_prob / z$prob_total
z$away_prob <- z$away_prob / z$prob_total
z$fav_prob <- pmax(z$home_prob, z$away_prob)
z <- z[order(-z$fav_prob),]
z$pick_pct_fav <- ifelse(z$home_prob >=.5, z$pick_pct_home, z$pick_pct_away)
z$fav <- ifelse(z$home_prob >=.5, as.character(z$home_teams), as.character(z$away_teams))
z$confidence <- seq(16,17-length(z$fav))
z
z <- select(z,confidence, fav, fav_prob, pick_pct_fav)
write.csv(z, file = "Picks2.csv", row.names = FALSE)
library(rvest)
library(tidyr)
library(dplyr)
web_page1 <- "https://football.fantasysports.yahoo.com/pickem/pickdistribution?gid=&week=12&type=c"
web_page2 <- "http://www.covers.com/odds/football/nfl-moneyline-odds.aspx"
html_page1 <- read_html(web_page1)
html_page2 <- read_html(web_page2)
# Read in data from yahoo
pick_pct_html <- html_nodes(html_page1, "dd")
pick_pct <- html_text(pick_pct_html)
even_index <- seq(2,length(pick_pct),2)
odd_index <- seq(1, length(pick_pct)-1, 2)
teams <- pick_pct[odd_index]
home_teams <- teams[grep("@",teams)]
away_teams <- teams[!grepl("@ ", teams)]
pick_pct <- pick_pct[even_index]
pick_pct_home <- pick_pct[match(home_teams,teams)]
home_teams <- gsub("@ ","",home_teams)
teams2_html <- html_nodes(html_page2, "strong")
teams2 <- html_text(teams2_html)
length(teams2) <- length(teams2) - 1
home_teams2 <- teams2[grep("@",teams2)]
#away_teams2 <- teams2[!grepl("@", teams2)]
home_teams2 <- gsub("@","",home_teams2)
covers_html <- html_nodes(html_page2, ".covers_top")
covers <- html_text(covers_html)
covers <- gsub("[\r\n]", "", covers)
covers_df <- data.frame(covers)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
# To use one of these functions on myDummy$country:
covers_df$covers <- trim(covers_df$covers)
covers_df <- Test <- separate(covers_df, col = "covers", into = c("away", "home"), sep = "\\s+")
x <- cbind.data.frame(home_teams, away_teams, pick_pct_home)
x$home_teams <- trim(x$home_teams)
x$pick_pct_home <- as.numeric(gsub("\\%", "", x$pick_pct_home))/100
x$pick_pct_away <- 1 - x$pick_pct_home
y <- cbind.data.frame(home_teams2, covers_df)
names(y)[names(y)=="home_teams2"] <- "home_teams"
y$home_teams <- gsub("N.Y. Jets", "New York (NYJ)", y$home_teams)
y$home_teams <- gsub("N.Y. Giants", "New York (NYG)", y$home_teams)
y$home <- gsub("OFF", -100, y$home)
y$away <- gsub("OFF", -100, y$away)
z <- merge(x,y,by = "home_teams")
winprob <- function(x){
ifelse(x <= 0, -x / (100-x), 100/(100+x))
}
z$home_prob <- winprob(as.numeric(as.character(z$home)))
z$away_prob <- winprob(as.numeric(as.character(z$away)))
z$prob_total <- z$home_prob + z$away_prob
z$home_prob <- z$home_prob / z$prob_total
z$away_prob <- z$away_prob / z$prob_total
z$fav_prob <- pmax(z$home_prob, z$away_prob)
z <- z[order(-z$fav_prob),]
z$pick_pct_fav <- ifelse(z$home_prob >=.5, z$pick_pct_home, z$pick_pct_away)
z$fav <- ifelse(z$home_prob >=.5, as.character(z$home_teams), as.character(z$away_teams))
z$confidence <- seq(16,17-length(z$fav))
z <- select(z,confidence, fav, fav_prob, pick_pct_fav)
z
write.csv(z, file = "Picks2.csv", row.names = FALSE)
getwd()
setwd("C:/Users/Zach/Desktop/Coursera/8 - Practical Machine Learning")
#load necessary packages & dataset
library(RCurl); library(caret); library(rpart); library(rpart.plot);
library(RColorBrewer); library(rattle); library(randomForest);
org_train <- read.csv("train.csv", na.strings=c("NA","#DIV/0!",""))
org_test <- read.csv("test.csv", na.strings=c("NA","#DIV/0!",""))
set.seed(456146)
#Keep only the numeric variables (except for classe)
training <- org_train[,which(lapply(org_train, class) %in% "numeric")]
training$classe <- org_train$classe
#Clean up number of variables by removing columns with a large percentage (>10%) of blanks or errors
training <- training[ lapply(training, function(x) sum(is.na(x)) / length(x) ) < 0.1 ]
#Splitting the original training set into a training and a cross validation set
inTrain <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
training <- training[inTrain, ]; testing <- training[-inTrain, ]
dim(training); dim(testing)
tree <- rpart(classe ~., data=training, method="class")
fancyRpartPlot(tree)
plot(tree)
text(tree)
tree
tree_trainpred <- predict(tree, training)
tree_trainpred <- predict(tree, training, type = "class")
head(tree_trainpred)
dim(tree_trainpred)
tree_trainpred
confusionMatrix(trainingPred, training$classe)
confusionMatrix(tree_trainpred, training$classe)
# Hold-out set accuracy
tree_test_pred <- predict(tree, training, type = "class")
confusionMatrix(tree_test_pred, training$classe)
# Hold-out set accuracy
tree_test_pred <- predict(tree, testing, type = "class")
confusionMatrix(tree_test_pred, testing$classe)
set.seed(456146)
#Keep only the numeric variables (except for classe)
training <- org_train[,which(lapply(org_train, class) %in% "numeric")]
training$classe <- org_train$classe
#Clean up number of variables by removing columns with a large percentage (>10%) of blanks or errors
training <- training[ lapply(training, function(x) sum(is.na(x)) / length(x) ) < 0.1 ]
#Splitting the original training set into a training and a cross validation set
inTrain <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
training <- training[inTrain, ]; testing <- training[-inTrain, ]
dim(training); dim(testing)
model_tree <- rpart(classe ~., data=training, method="class")
# Visualize the decision tree
plot(model_tree)
text(model_tree)
# A fancier version of the plot
fancyRpartPlot(model_tree)
# Training set accuracy
tree_train_pred <- predict(model_tree, training, type = "class")
confusionMatrix(tree_train_pred, training$classe)
# Hold-out set accuracy
tree_test_pred <- predict(tree, testing, type = "class")
confusionMatrix(tree_test_pred, testing$classe)
model_rf <- train(classe ~., method="rf", data=training, trControl=trainControl(method='cv'), number=5, allowParallel=TRUE )
model_rf
varImpPlot(model_fe)
varImpPlot(model_rf)
model_rf$finalModel
trainingPred <- predict(model_rf, training)
confusionMatrix(trainingPred, training$classe)
trainingPred <- predict(model_rf, testing)
confusionMatrix(trainingPred, testing$classe)
testingPred <- predict(model_rf, test)
testingPred <- predict(model_rf, org_test)
testingPred
confusionMatrix(tree_train_pred, training$classe)$overall
confusionMatrix(tree_test_pred, testing$classe)$overall
varImp(model_rf)
varImp(model_rf)[1:5]
head(varImp(model_rf))
head(varImp(model_rf)$importance)
rownames(varImp(model_rf)$importance)[1:5]
rownames(varImp(model_tree)$importance)[1:5]
text(model_tree)
model_tree
model_tree$variable.importance
head(model_tree$variable.importance)
row_names(model_tree$variable.importance)
rownames(model_tree$variable.importance)
